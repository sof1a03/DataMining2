{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from textblob import TextBlob\n",
    "import pandas as pd \n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"train_reviews.csv\")\n",
    "trainsentences = traindf[\"Review\"].values\n",
    "trainlabels = traindf[\"Label\"].values\n",
    "testdf = pd.read_csv(\"test_reviews.csv\")\n",
    "testsentences = testdf[\"Review\"].values\n",
    "testlabels = testdf[\"Label\"].values \n",
    "\n",
    "with open(\"refined_english_stopwords.txt\", \"r\") as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    words = text.split()   \n",
    "    words = [word for word in words if word not in stop_words]    \n",
    "    text = ' '.join(words).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_sentences_preprocessed = [preprocess_text(sentence) for sentence in trainsentences]\n",
    "test_sentences_preprocessed = [preprocess_text(sentence) for sentence in testsentences]\n",
    "train_df_preprocessed = pd.DataFrame({'Review': train_sentences_preprocessed,'Label': trainlabels})\n",
    "test_df_preprocessed = pd.DataFrame({'Review': test_sentences_preprocessed,'Label': testlabels})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentiment_features(df):\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    for review in df['Review']:\n",
    "        blob = TextBlob(review)\n",
    "        polarity.append(blob.sentiment.polarity)\n",
    "        subjectivity.append(blob.sentiment.subjectivity)\n",
    "    df['polarity'] = polarity\n",
    "    df['subjectivity'] = subjectivity\n",
    "    return df\n",
    "x_train_sentiment = extract_sentiment_features(train_df_preprocessed)\n",
    "x_test_sentiment = extract_sentiment_features(test_df_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 6967)\n"
     ]
    }
   ],
   "source": [
    "trainsentences = train_df_preprocessed[\"Review\"].values\n",
    "y_train = train_df_preprocessed[\"Label\"].values\n",
    "\n",
    "testsentences = test_df_preprocessed[\"Review\"].values\n",
    "y_test = test_df_preprocessed[\"Label\"].values\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1)) # ngram range for specifying unigrams and bigrams (1,1) - unigram, (2,2) - bigram, (1,2) - both\n",
    "X_train = vectorizer.fit_transform(trainsentences)\n",
    "print(X_train.shape)\n",
    "X_test = vectorizer.transform(testsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, y_test):#, exclude_sentiment=False):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    dict={'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi squared test for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(640, 2000)\n",
      "{'accuracy': 0.86875, 'precision': 0.8715950244056054, 'recall': 0.8687499999999999, 'f1': 0.8684982975226019}\n",
      "Selected Features: ['abassador' 'accept' 'accepted' ... 'yunan' 'zone' 'zoo']\n",
      "Test Set Accuracy: 0.86875\n"
     ]
    }
   ],
   "source": [
    "# Perform Chi-squared feature selection\n",
    "chi2_selector = SelectKBest(chi2, k=2000) #2000 features perform best\n",
    "X_train_chi2 = chi2_selector.fit_transform(X_train, y_train)\n",
    "print(X_train_chi2.shape)\n",
    "X_test_chi2 = chi2_selector.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_chi2, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = nb_model.predict(X_test_chi2)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Get the feature names for the selected features\n",
    "selected_feature_indices = chi2_selector.get_support(indices=True)\n",
    "selected_feature_names = vectorizer.get_feature_names_out()[selected_feature_indices]\n",
    "\n",
    "print(f\"Selected Features: {selected_feature_names}\")\n",
    "print(f\"Test Set Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.86875, 'precision': 0.8715950244056054, 'recall': 0.8687499999999999, 'f1': 0.8684982975226019}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "multinomial_naive_bayes.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = multinomial_naive_bayes.predict(X_test)\n",
    "#print(evaluate_model(multinomial_naive_bayes, x_train, x_test, y_train, y_test))\n",
    "print(evaluate_model(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Log Probabilities for Multinomial Naive Bayes method1:\n",
      "Number of features:  2000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of features: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(log_probs[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create DataFrames for each class\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m deceptive_features \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFeature\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mLog Prob\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m genuine_features \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_names, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLog Prob\u001b[39m\u001b[38;5;124m'\u001b[39m: log_probs[\u001b[38;5;241m0\u001b[39m]})\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sort by log probabilities\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[0;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[0;32m    774\u001b[0m     )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[1;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[0;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[1;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[1;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    682\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Get log probabilities\n",
    "print(\"Feature Log Probabilities for Multinomial Naive Bayes method1:\")\n",
    "\n",
    "log_probs = multinomial_naive_bayes.feature_log_prob_\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Number of features: \", len(log_probs[1]))\n",
    "# Create DataFrames for each class\n",
    "deceptive_features = pd.DataFrame({'Feature': feature_names, 'Log Prob': log_probs[1]})\n",
    "genuine_features = pd.DataFrame({'Feature': feature_names, 'Log Prob': log_probs[0]})\n",
    "\n",
    "# Sort by log probabilities\n",
    "deceptive_features = deceptive_features.sort_values(by='Log Prob', ascending=False)\n",
    "genuine_features = genuine_features.sort_values(by='Log Prob', ascending=False)\n",
    "\n",
    "# Print the top 5 features for each class\n",
    "print(\"Top features for Deceptive Reviews:\")\n",
    "print(deceptive_features.head())\n",
    "\n",
    "print(\"\\nTop features for Genuine Reviews:\")\n",
    "print(genuine_features.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for splits: [1.     0.875  0.875  0.75   0.8125 0.875  0.8125 0.875  0.875  0.875 ]\n",
      "0.86 accuracy with a standard deviation of 0.06\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(multinomial_naive_bayes, X_test, y_test, cv=10, scoring = \"accuracy\") #try f1_macro\n",
    "print(\"accuracy for splits: {}\".format(scores))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 1.0, 'fit_prior': True}\n",
      "Best Cross-Validation Score: 0.846875\n",
      "Test Set Score: 0.86875\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Laplace smoothing parameter\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(multinomial_naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8063\n",
      "Precision: 0.8075\n",
      "Recall: 0.8063\n",
      "F1 Score: 0.8061\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "print(evaluate_model(logistic_regression, x_train, x_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Features for Genuine Reviews:\n",
      "         feature  coefficient\n",
      "5783        star     1.949148\n",
      "6897       world     1.372054\n",
      "5876      street     1.319051\n",
      "6382         try     1.155370\n",
      "1271  conference     1.130359\n",
      "\n",
      "Important Features for Deceptive Reviews:\n",
      "      feature  coefficient\n",
      "4624   prices    -1.745096\n",
      "2327  finally    -1.699906\n",
      "1566  decided    -1.381042\n",
      "1020  chicago    -1.325427\n",
      "3622   luxury    -1.236684\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients\n",
    "print(\"Feature Coefficients for Logistic Regression method1:\")\n",
    "coefficients = logistic_regression.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "\n",
    "# Separate features for genuine and deceptive reviews\n",
    "genuine_features = coef_df[coef_df['coefficient'] > 0].sort_values(by='coefficient', ascending=False)\n",
    "deceptive_features = coef_df[coef_df['coefficient'] < 0].sort_values(by='coefficient')\n",
    "\n",
    "# Display the results\n",
    "print(\"Important Features for Genuine Reviews:\")\n",
    "print(genuine_features.head())\n",
    "\n",
    "print(\"\\nImportant Features for Deceptive Reviews:\")\n",
    "print(deceptive_features.head())\n",
    "\n",
    "print(\"Feature Importance for Logistic Regression method2:\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10}\n",
      "Best Cross-Validation Score: 0.8046875\n",
      "Test Set Score: 0.76875\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6250\n",
      "Precision: 0.6270\n",
      "Recall: 0.6250\n",
      "F1 Score: 0.6235\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature  importance\n",
      "1020  chicago    0.192117\n",
      "1358     cool    0.031134\n",
      "5418   sheets    0.031132\n",
      "5839    still    0.028451\n",
      "3676     many    0.026788\n",
      "\n",
      "        feature  importance\n",
      "2333    finger         0.0\n",
      "2332    finest         0.0\n",
      "2331  finejust         0.0\n",
      "2330      fine         0.0\n",
      "6966       zoo         0.0\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = decision_tree.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)\n",
    "\n",
    "# Get the top 5 important features for deceptive reviews\n",
    "top_5_genuine_review_terms = importance_df.tail(5)  # or use .head(5) based on your label encoding\n",
    "print(\"\\n\", top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'X' parameter of permutation_importance must be an array-like. Got <160x6967 sparse matrix of type '<class 'numpy.int64'>'\n\twith 10744 stored elements in Compressed Sparse Row format> instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the sparse matrix X_test to a dense format\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_test_dense \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m----> 3\u001b[0m perm_importance \u001b[38;5;241m=\u001b[39m \u001b[43mpermutation_importance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecision_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_repeats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Display the importance scores for the positive class\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:203\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m to_ignore \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcls\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    201\u001b[0m params \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m params\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m to_ignore}\n\u001b[1;32m--> 203\u001b[0m \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparameter_constraints\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__qualname__\u001b[39;49m\n\u001b[0;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\sofia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'X' parameter of permutation_importance must be an array-like. Got <160x6967 sparse matrix of type '<class 'numpy.int64'>'\n\twith 10744 stored elements in Compressed Sparse Row format> instead."
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix X_test to a dense format\n",
    "X_test_dense = X_test.toarray()\n",
    "perm_importance = permutation_importance(decision_tree, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Display the importance scores for the positive class\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance genuine:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "print(\"\\nPermutation Importance deceptive:\")\n",
    "print(importance_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'ccp_alpha': 0.0, 'criterion': 'gini', 'max_depth': 8, 'min_samples_leaf': 6, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.7203125\n",
      "Test Set Score: 0.68125\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'ccp_alpha': np.linspace(0, 0.1, 11) \n",
    "}\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000\n",
      "Precision: 0.8000\n",
      "Recall: 0.8000\n",
      "F1 Score: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = random_forest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "f1 = f1_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature  importance\n",
      "1020     chicago    0.030663\n",
      "3544    location    0.010557\n",
      "5585       smell    0.007676\n",
      "2180  experience    0.007068\n",
      "2677       great    0.006658\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.toarray() \n",
    "# X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance genuine:\n",
      "       Feature  Importance\n",
      "2005  elevator     0.02375\n",
      "1020   chicago     0.02250\n",
      "4617    prices     0.02000\n",
      "3544  location     0.01625\n",
      "5580     small     0.01375\n",
      "\n",
      "Permutation Importance deceptive:\n",
      "      Feature  Importance\n",
      "3485     like    -0.01125\n",
      "349   arrived    -0.01125\n",
      "6319   travel    -0.01250\n",
      "4372   people    -0.01250\n",
      "2501    front    -0.01500\n"
     ]
    }
   ],
   "source": [
    "perm_importance = permutation_importance(random_forest, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Display the importance scores for the positive class\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance genuine:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "print(\"\\nPermutation Importance deceptive:\")\n",
    "print(importance_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.8359375\n",
      "Test Set Score: 0.8125\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " max_depth               10.0\n",
      "max_features            sqrt\n",
      "min_samples_leaf           2\n",
      "min_samples_split          5\n",
      "n_estimators             100\n",
      "oob_score            0.83125\n",
      "Name: 23, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = RandomForestClassifier(oob_score=True, random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Record OOB score\n",
    "    oob_score = model.oob_score_\n",
    "    results.append({**params, 'oob_score': oob_score})\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best hyperparameters based on OOB score\n",
    "best_params = results_df.loc[results_df['oob_score'].idxmax()]\n",
    "print(\"Best Hyperparameters:\\n\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"train_reviews.csv\")\n",
    "trainsentences = traindf[\"Review\"].values\n",
    "trainlabels = traindf[\"Label\"].values\n",
    "testdf = pd.read_csv(\"test_reviews.csv\")\n",
    "testsentences = testdf[\"Review\"].values\n",
    "testlabels = testdf[\"Label\"].values \n",
    "\n",
    "with open(\"english\", \"r\") as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    words = text.split()   \n",
    "    words = [word for word in words if word not in stop_words]    \n",
    "    text = ' '.join(words).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_sentences_preprocessed = [preprocess_text(sentence) for sentence in trainsentences]\n",
    "test_sentences_preprocessed = [preprocess_text(sentence) for sentence in testsentences]\n",
    "train_df_preprocessed = pd.DataFrame({'Review': train_sentences_preprocessed,'Label': trainlabels})\n",
    "test_df_preprocessed = pd.DataFrame({'Review': test_sentences_preprocessed,'Label': testlabels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsentences = train_df_preprocessed[\"Review\"].values\n",
    "y_train = train_df_preprocessed[\"Label\"].values\n",
    "\n",
    "testsentences = test_df_preprocessed[\"Review\"].values\n",
    "y_test = test_df_preprocessed[\"Label\"].values\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2)) # ngram range for specifying unigrams and bigrams (1,1) - unigram, (2,2) - bigram, (1,2) - both\n",
    "X_train = vectorizer.fit_transform(trainsentences)\n",
    "X_test = vectorizer.transform(testsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test:\n",
    "    if i != \"Negative Deceptive\" and i != \"Negative Truthful\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7688\n",
      "Precision: 0.7785\n",
      "Recall: 0.7688\n",
      "F1 Score: 0.7667\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "multinomial_naive_bayes.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = multinomial_naive_bayes.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for splits: [0.8125 0.5625 0.6875 0.5625 0.625  0.625  0.875  0.5625 0.6875 0.8125]\n",
      "0.68 accuracy with a standard deviation of 0.11\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(multinomial_naive_bayes, X_test, y_test, cv=10, scoring = \"accuracy\") #try f1_macro\n",
    "print(\"accuracy for splits: {}\".format(scores))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'fit_prior': True}\n",
      "Best Cross-Validation Score: 0.7515625\n",
      "Test Set Score: 0.7625\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Laplace smoothing parameter\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(multinomial_naive_bayes, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6625\n",
      "Precision: 0.7098\n",
      "Recall: 0.6625\n",
      "F1 Score: 0.6423\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "accuracy = accuracy_score(y_test ,y_pred)\n",
    "precision = precision_score(y_test ,y_pred,average='weighted')\n",
    "recall = recall_score(y_test ,y_pred,average='weighted')\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Features for Genuine Reviews:\n",
      "               feature  coefficient\n",
      "36694         th floor     2.482137\n",
      "4121      booked hotel     1.818765\n",
      "22268       many times     1.546209\n",
      "21175   location worth     1.228648\n",
      "23457      much better     1.122411\n",
      "...                ...          ...\n",
      "22699  michigan avenue     0.030777\n",
      "17202        hot water     0.021508\n",
      "11003      duvet cover     0.016028\n",
      "25446        one night     0.007881\n",
      "17687       hotel room     0.004241\n",
      "\n",
      "[72 rows x 2 columns]\n",
      "\n",
      "Important Features for Deceptive Reviews:\n",
      "                  feature   coefficient\n",
      "6315   chicago millennium -3.247623e+00\n",
      "11070          east hotel -2.298146e+00\n",
      "21711        luxury hotel -2.260378e+00\n",
      "17292       hotel chicago -2.242835e+00\n",
      "7896       conrad chicago -2.175113e+00\n",
      "...                   ...           ...\n",
      "9534           desk clerk -3.280306e-02\n",
      "40527         waste money -2.084544e-02\n",
      "2690            back room -1.165412e-02\n",
      "17498          hotel last -4.714746e-03\n",
      "31953          seem paper -1.312044e-07\n",
      "\n",
      "[125 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients\n",
    "coefficients = logistic_regression.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "\n",
    "# Separate features for genuine and deceptive reviews\n",
    "genuine_features = coef_df[coef_df['coefficient'] > 0].sort_values(by='coefficient', ascending=False)\n",
    "deceptive_features = coef_df[coef_df['coefficient'] < 0].sort_values(by='coefficient')\n",
    "\n",
    "# Display the results\n",
    "print(\"Important Features for Genuine Reviews:\")\n",
    "print(genuine_features)\n",
    "\n",
    "print(\"\\nImportant Features for Deceptive Reviews:\")\n",
    "print(deceptive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10}\n",
      "Best Cross-Validation Score: 0.7390625\n",
      "Test Set Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6500\n",
      "Precision: 0.6881\n",
      "Recall: 0.6500\n",
      "F1 Score: 0.6313\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature  importance\n",
      "6264        chicago hotel    0.052528\n",
      "17292       hotel chicago    0.052329\n",
      "6315   chicago millennium    0.035386\n",
      "21711        luxury hotel    0.028640\n",
      "11070          east hotel    0.025242\n",
      "\n",
      "                 feature  importance\n",
      "14159     forward using         0.0\n",
      "14160  forward vacation         0.0\n",
      "14161      forward view         0.0\n",
      "14162      forward wifi         0.0\n",
      "42435        zoo second         0.0\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = decision_tree.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)\n",
    "\n",
    "# Get the top 5 important features for deceptive reviews\n",
    "top_5_genuine_review_terms = importance_df.tail(5)  # or use .head(5) based on your label encoding\n",
    "print(\"\\n\", top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.7171875\n",
      "Test Set Score: 0.6375\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4, 6]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5938\n",
      "Precision: 0.7055\n",
      "Recall: 0.5938\n",
      "F1 Score: 0.5298\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = random_forest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "f1 = f1_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  importance\n",
      "17292     hotel chicago    0.011261\n",
      "6264      chicago hotel    0.010452\n",
      "29003   recently stayed    0.008170\n",
      "32746  sheraton chicago    0.004932\n",
      "7896     conrad chicago    0.004806\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.7625\n",
      "Test Set Score: 0.68125\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
