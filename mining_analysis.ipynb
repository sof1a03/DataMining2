{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "import pandas as pd \n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.inspection import permutation_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"train_reviews.csv\")\n",
    "trainsentences = traindf[\"Review\"].values\n",
    "trainlabels = traindf[\"Label\"].values\n",
    "testdf = pd.read_csv(\"test_reviews.csv\")\n",
    "testsentences = testdf[\"Review\"].values\n",
    "testlabels = testdf[\"Label\"].values \n",
    "\n",
    "with open(\"refined_english_stopwords.txt\", \"r\") as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    words = text.split()   \n",
    "    words = [word for word in words if word not in stop_words]    \n",
    "    text = ' '.join(words).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_sentences_preprocessed = [preprocess_text(sentence) for sentence in trainsentences]\n",
    "test_sentences_preprocessed = [preprocess_text(sentence) for sentence in testsentences]\n",
    "train_df_preprocessed = pd.DataFrame({'Review': train_sentences_preprocessed,'Label': trainlabels})\n",
    "test_df_preprocessed = pd.DataFrame({'Review': test_sentences_preprocessed,'Label': testlabels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsentences = train_df_preprocessed[\"Review\"].values\n",
    "y_train = train_df_preprocessed[\"Label\"].values\n",
    "\n",
    "testsentences = test_df_preprocessed[\"Review\"].values\n",
    "y_test = test_df_preprocessed[\"Label\"].values\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    " # ngram range for specifying unigrams and bigrams (1,1) - unigram, (2,2) - bigram, (1,2) - both\n",
    "X_train = vectorizer.fit_transform(trainsentences)\n",
    "X_test = vectorizer.transform(testsentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_pred, y_test):#, exclude_sentiment=False):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='macro')\n",
    "    recall = recall_score(y_test, y_pred, average='macro')\n",
    "    f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    dict={'accuracy':accuracy, 'precision':precision, 'recall':recall, 'f1':f1}\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "#let's remember that we have computed the predictions for the four models:\n",
    "# multinomial_naive_bayes, logistic_regression, decision_tree, random_forest\n",
    "\n",
    "def contingency_matrics(y_true, pred1, pred2):\n",
    "    # Contingency Table\n",
    "    # Here, we compare both model predictions, not against y_true but against each other\n",
    "    a = np.sum((pred1 == y_true) & (pred2 == y_true))  # Both models correct\n",
    "    b = np.sum((pred1 == y_true) & (pred2 != y_true))  # Model 1 correct, Model 2 incorrect\n",
    "    c = np.sum((pred1 != y_true) & (pred2 == y_true))  # Model 1 incorrect, Model 2 correct\n",
    "    d = np.sum((pred1 != y_true) & (pred2 != y_true))  # Both models incorrect\n",
    "\n",
    "    # Print the contingency table\n",
    "    contingency_matrix = np.array([[a, b], [c, d]])\n",
    "    return contingency_matrix\n",
    "\n",
    "# Helper function to perform McNemar's test and print the contingency table\n",
    "def mcnemar_test(contingency_matrix):\n",
    "    \n",
    "    # Perform McNemar's test (you could also try exact=True for small samples)\n",
    "    mcnemar_r = mcnemar(contingency_matrix, exact=False, correction=True)\n",
    "    chi2 = mcnemar_r.statistic  # Access the test statistic\n",
    "    p_value = mcnemar_r.pvalue  # Access the p-value\n",
    "    return chi2, p_value\n",
    "\n",
    "def single_pairing_test(y_true, pred1, pred2, model_name1, model_name2):\n",
    "    \"\"\"\n",
    "    Compare three models pairwise using McNemar's test.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: Ground truth labels --> column 'post' from the dataset\n",
    "    - pred1: Predictions from Model 1 \n",
    "    - pred2: Predictions from Model 2\n",
    "\n",
    "    Returns:\n",
    "    - chi-squared statistics and p-values for pairwise comparisons, including a print statement showing if Hâ‚€ is accepted or rejected.\n",
    "    \"\"\"\n",
    "    cont_m=contingency_matrics(y_true, pred1, pred2)\n",
    "    chi2, p_value = mcnemar_test(cont_m)\n",
    "    print(f'Contingency Table:\\n{cont_m}')\n",
    "    print(f'Chi-squared: {chi2}')\n",
    "    print(f'p-value: {p_value}')\n",
    "    if p_value < (0.05/4):\n",
    "        print(\"The difference in performance is statistically significant.\\nReject the null hypothesis.\\n\")\n",
    "        print(f\"This means that {model_name1}'s accuracy is significantly different from {model_name2}'s accuracy.\")\n",
    "    else:\n",
    "        print(\"There's no statistically significant difference in performance.\\nAccept the null hypothesis.\\n\")\n",
    "        print(f\"This means that {model_name1}'s accuracy is no significantly different from {model_name2}'s accuracy.\")\n",
    "    \n",
    "    return cont_m, chi2, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chi squared test for feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Features: ['abassador' 'accept' 'accepted' ... 'yunan' 'zone' 'zoo']\n",
      "Test Set Accuracy: 0.86875\n"
     ]
    }
   ],
   "source": [
    "# Perform Chi-squared feature selection\n",
    "chi2_selector = SelectKBest(chi2, k=2000) #2000 features perform best\n",
    "X_train_chi2 = chi2_selector.fit_transform(X_train, y_train)\n",
    "#print(X_train_chi2.shape)\n",
    "X_test_chi2 = chi2_selector.transform(X_test)\n",
    "\n",
    "# Train a Multinomial Naive Bayes model\n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(X_train_chi2, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_chi2 = nb_model.predict(X_test_chi2)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_chi2)\n",
    "\n",
    "# Get the feature names for the selected features\n",
    "selected_feature_indices = chi2_selector.get_support(indices=True)\n",
    "selected_feature_names = vectorizer.get_feature_names_out()[selected_feature_indices]\n",
    "\n",
    "print(f\"Selected Features: {selected_feature_names}\")\n",
    "print(f\"Test Set Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.86875, 'precision': 0.8734768159518911, 'recall': 0.86875, 'f1': 0.8683333986441475}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "multinomial_naive_bayes.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_bayes = multinomial_naive_bayes.predict(X_test)\n",
    "dict_bayes=evaluate_model(y_pred_bayes, y_test)\n",
    "print(dict_bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6967\n",
      "Top features for Deceptive Reviews:\n",
      "     Feature  Log Prob\n",
      "6758      we -3.908500\n",
      "5141    room -4.044659\n",
      "2955   hotel -4.122824\n",
      "4078     not -4.218297\n",
      "6170    they -4.402409\n",
      "\n",
      "Top features for Genuine Reviews:\n",
      "      Feature  Log Prob\n",
      "5141     room -3.832078\n",
      "2955    hotel -3.911186\n",
      "6758       we -3.919316\n",
      "4078      not -4.107368\n",
      "1020  chicago -4.597608\n"
     ]
    }
   ],
   "source": [
    "# Get log probabilities\n",
    "log_probs = multinomial_naive_bayes.feature_log_prob_\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(len(log_probs[0]))\n",
    "# Create DataFrames for each class\n",
    "deceptive_features = pd.DataFrame({'Feature': feature_names, 'Log Prob': log_probs[1]})\n",
    "genuine_features = pd.DataFrame({'Feature': feature_names, 'Log Prob': log_probs[0]})\n",
    "\n",
    "# Sort by log probabilities\n",
    "deceptive_features = deceptive_features.sort_values(by='Log Prob', ascending=False)\n",
    "genuine_features = genuine_features.sort_values(by='Log Prob', ascending=False)\n",
    "\n",
    "# Print the top 10 features for each class\n",
    "print(\"Top features for Deceptive Reviews:\")\n",
    "print(deceptive_features.head())\n",
    "\n",
    "print(\"\\nTop features for Genuine Reviews:\")\n",
    "print(genuine_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for splits: [1.     0.875  0.875  0.75   0.8125 0.875  0.8125 0.875  0.875  0.875 ]\n",
      "0.86 accuracy with a standard deviation of 0.06\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(multinomial_naive_bayes, X_test, y_test, cv=10, scoring = \"accuracy\") #try f1_macro\n",
    "print(\"accuracy for splits: {}\".format(scores))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 1.0, 'fit_prior': True}\n",
      "Best Cross-Validation Score: 0.846875\n",
      "Test Set Score: 0.86875\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Laplace smoothing parameter\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(multinomial_naive_bayes, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.80625, 'precision': 0.8074509803921568, 'recall': 0.80625, 'f1': 0.8060606060606061}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_log = logistic_regression.predict(X_test)\n",
    "dict_log=evaluate_model(y_pred_log, y_test)\n",
    "print(dict_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Features for Genuine Reviews:\n",
      "         feature  coefficient\n",
      "5783        star     1.949962\n",
      "6897       world     1.375677\n",
      "5876      street     1.318635\n",
      "6382         try     1.156914\n",
      "1271  conference     1.129812\n",
      "\n",
      "Important Features for Deceptive Reviews:\n",
      "      feature  coefficient\n",
      "4624   prices    -1.744230\n",
      "2327  finally    -1.699424\n",
      "1566  decided    -1.381841\n",
      "1020  chicago    -1.325233\n",
      "3622   luxury    -1.235582\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients\n",
    "coefficients = logistic_regression.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "\n",
    "# Separate features for genuine and deceptive reviews\n",
    "genuine_features = coef_df[coef_df['coefficient'] > 0].sort_values(by='coefficient', ascending=False)\n",
    "deceptive_features = coef_df[coef_df['coefficient'] < 0].sort_values(by='coefficient')\n",
    "\n",
    "# Display the results\n",
    "print(\"Important Features for Genuine Reviews:\")\n",
    "print(genuine_features.head())\n",
    "\n",
    "print(\"\\nImportant Features for Deceptive Reviews:\")\n",
    "print(deceptive_features.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10}\n",
      "Best Cross-Validation Score: 0.803125\n",
      "Test Set Score: 0.76875\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.625, 'precision': 0.626984126984127, 'recall': 0.625, 'f1': 0.6235294117647059}\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_ct = decision_tree.predict(X_test)\n",
    "dict_ct=evaluate_model(y_pred_ct, y_test)\n",
    "print(dict_ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature  importance\n",
      "1020  chicago    0.192117\n",
      "1358     cool    0.031134\n",
      "5418   sheets    0.031132\n",
      "5839    still    0.028451\n",
      "3676     many    0.026788\n",
      "\n",
      "        feature  importance\n",
      "2333    finger         0.0\n",
      "2332    finest         0.0\n",
      "2331  finejust         0.0\n",
      "2330      fine         0.0\n",
      "6966       zoo         0.0\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = decision_tree.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)\n",
    "\n",
    "# Get the top 5 important features for deceptive reviews\n",
    "top_5_genuine_review_terms = importance_df.tail(5)  # or use .head(5) based on your label encoding\n",
    "print(\"\\n\", top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance genuine:\n",
      "       Feature  Importance\n",
      "1020   chicago    0.081250\n",
      "3676      many    0.018750\n",
      "3794  michigan    0.015000\n",
      "1358      cool    0.011875\n",
      "6862    within    0.010625\n",
      "\n",
      "Permutation Importance deceptive:\n",
      "     Feature  Importance\n",
      "4025   night   -0.004375\n",
      "4049     non   -0.006875\n",
      "1468  crumbs   -0.007500\n",
      "348   arrive   -0.011250\n",
      "4358     pay   -0.014375\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Convert the sparse matrix X_test to a dense format\n",
    "X_test_dense = X_test.toarray()\n",
    "perm_importance = permutation_importance(decision_tree, X_test_dense, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Display the importance scores for the positive class\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance genuine:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "print(\"\\nPermutation Importance deceptive:\")\n",
    "print(importance_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4, 6],  # Minimum number of samples required to be at a leaf node\n",
    "    'ccp_alpha': np.linspace(0, 0.1, 11) \n",
    "}\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8000\n",
      "Precision: 0.8000\n",
      "Recall: 0.8000\n",
      "F1 Score: 0.8000\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred_rf = random_forest.predict(X_test)\n",
    "dict_rf=evaluate_model(y_pred_rf, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         feature  importance\n",
      "1020     chicago    0.030663\n",
      "3544    location    0.010557\n",
      "5585       smell    0.007676\n",
      "2180  experience    0.007068\n",
      "2677       great    0.006658\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = X_train.toarray() \n",
    "# X_test = X_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Permutation Importance genuine:\n",
      "       Feature  Importance\n",
      "2005  elevator     0.02375\n",
      "1020   chicago     0.02250\n",
      "4617    prices     0.02000\n",
      "3544  location     0.01625\n",
      "5580     small     0.01375\n",
      "\n",
      "Permutation Importance deceptive:\n",
      "      Feature  Importance\n",
      "3485     like    -0.01125\n",
      "349   arrived    -0.01125\n",
      "6319   travel    -0.01250\n",
      "4372   people    -0.01250\n",
      "2501    front    -0.01500\n"
     ]
    }
   ],
   "source": [
    "perm_importance = permutation_importance(random_forest, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "# Display the importance scores for the positive class\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': perm_importance.importances_mean\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nPermutation Importance genuine:\")\n",
    "print(importance_df.head())\n",
    "\n",
    "print(\"\\nPermutation Importance deceptive:\")\n",
    "print(importance_df.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.8359375\n",
      "Test Set Score: 0.8125\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOB score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      " max_depth               10.0\n",
      "max_features            sqrt\n",
      "min_samples_leaf           2\n",
      "min_samples_split          5\n",
      "n_estimators             100\n",
      "oob_score            0.83125\n",
      "Name: 23, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2],  # Minimum number of samples required to be at a leaf node\n",
    "    'ccp_alpha': np.linspace(0, 0.1, 11) \n",
    "}\n",
    "\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Loop through hyperparameter combinations\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = RandomForestClassifier(oob_score=True, random_state=42, **params)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Record OOB score\n",
    "    oob_score = model.oob_score_\n",
    "    results.append({**params, 'oob_score': oob_score})\n",
    "\n",
    "# Convert results to DataFrame for easier analysis\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Find the best hyperparameters based on OOB score\n",
    "best_params = results_df.loc[results_df['oob_score'].idxmax()]\n",
    "print(\"Best Hyperparameters:\\n\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = pd.read_csv(\"train_reviews.csv\")\n",
    "trainsentences = traindf[\"Review\"].values\n",
    "trainlabels = traindf[\"Label\"].values\n",
    "testdf = pd.read_csv(\"test_reviews.csv\")\n",
    "testsentences = testdf[\"Review\"].values\n",
    "testlabels = testdf[\"Label\"].values \n",
    "\n",
    "with open(\"english\", \"r\") as file:\n",
    "    stop_words = set(file.read().splitlines())\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text) \n",
    "    words = text.split()   \n",
    "    words = [word for word in words if word not in stop_words]    \n",
    "    text = ' '.join(words).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_sentences_preprocessed = [preprocess_text(sentence) for sentence in trainsentences]\n",
    "test_sentences_preprocessed = [preprocess_text(sentence) for sentence in testsentences]\n",
    "train_df_preprocessed = pd.DataFrame({'Review': train_sentences_preprocessed,'Label': trainlabels})\n",
    "test_df_preprocessed = pd.DataFrame({'Review': test_sentences_preprocessed,'Label': testlabels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsentences = train_df_preprocessed[\"Review\"].values\n",
    "y_train = train_df_preprocessed[\"Label\"].values\n",
    "\n",
    "testsentences = test_df_preprocessed[\"Review\"].values\n",
    "y_test = test_df_preprocessed[\"Label\"].values\n",
    "\n",
    "# vectorizing\n",
    "vectorizer = CountVectorizer(ngram_range=(2,2)) # ngram range for specifying unigrams and bigrams (1,1) - unigram, (2,2) - bigram, (1,2) - both\n",
    "X_train = vectorizer.fit_transform(trainsentences)\n",
    "X_test = vectorizer.transform(testsentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in y_test:\n",
    "    if i != \"Negative Deceptive\" and i != \"Negative Truthful\":\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7688\n",
      "Precision: 0.7785\n",
      "Recall: 0.7688\n",
      "F1 Score: 0.7667\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "multinomial_naive_bayes.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = multinomial_naive_bayes.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for splits: [0.8125 0.5625 0.6875 0.5625 0.625  0.625  0.875  0.5625 0.6875 0.8125]\n",
      "0.68 accuracy with a standard deviation of 0.11\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(multinomial_naive_bayes, X_test, y_test, cv=10, scoring = \"accuracy\") #try f1_macro\n",
    "print(\"accuracy for splits: {}\".format(scores))\n",
    "print(\"%0.2f accuracy with a standard deviation of %0.2f\" % (scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'alpha': 0.1, 'fit_prior': True}\n",
      "Best Cross-Validation Score: 0.7515625\n",
      "Test Set Score: 0.7625\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 1.5, 2.0],  # Laplace smoothing parameter\n",
    "    'fit_prior': [True, False]\n",
    "}\n",
    "multinomial_naive_bayes = MultinomialNB()\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(multinomial_naive_bayes, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6625\n",
      "Precision: 0.7098\n",
      "Recall: 0.6625\n",
      "F1 Score: 0.6423\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "logistic_regression.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = logistic_regression.predict(X_test)\n",
    "accuracy = accuracy_score(y_test ,y_pred)\n",
    "precision = precision_score(y_test ,y_pred,average='weighted')\n",
    "recall = recall_score(y_test ,y_pred,average='weighted')\n",
    "f1 = f1_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important Features for Genuine Reviews:\n",
      "               feature  coefficient\n",
      "36694         th floor     2.482137\n",
      "4121      booked hotel     1.818765\n",
      "22268       many times     1.546209\n",
      "21175   location worth     1.228648\n",
      "23457      much better     1.122411\n",
      "...                ...          ...\n",
      "22699  michigan avenue     0.030777\n",
      "17202        hot water     0.021508\n",
      "11003      duvet cover     0.016028\n",
      "25446        one night     0.007881\n",
      "17687       hotel room     0.004241\n",
      "\n",
      "[72 rows x 2 columns]\n",
      "\n",
      "Important Features for Deceptive Reviews:\n",
      "                  feature   coefficient\n",
      "6315   chicago millennium -3.247623e+00\n",
      "11070          east hotel -2.298146e+00\n",
      "21711        luxury hotel -2.260378e+00\n",
      "17292       hotel chicago -2.242835e+00\n",
      "7896       conrad chicago -2.175113e+00\n",
      "...                   ...           ...\n",
      "9534           desk clerk -3.280306e-02\n",
      "40527         waste money -2.084544e-02\n",
      "2690            back room -1.165412e-02\n",
      "17498          hotel last -4.714746e-03\n",
      "31953          seem paper -1.312044e-07\n",
      "\n",
      "[125 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get the coefficients\n",
    "coefficients = logistic_regression.coef_[0]\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "coef_df = pd.DataFrame({'feature': feature_names, 'coefficient': coefficients})\n",
    "\n",
    "# Separate features for genuine and deceptive reviews\n",
    "genuine_features = coef_df[coef_df['coefficient'] > 0].sort_values(by='coefficient', ascending=False)\n",
    "deceptive_features = coef_df[coef_df['coefficient'] < 0].sort_values(by='coefficient')\n",
    "\n",
    "# Display the results\n",
    "print(\"Important Features for Genuine Reviews:\")\n",
    "print(genuine_features)\n",
    "\n",
    "print(\"\\nImportant Features for Deceptive Reviews:\")\n",
    "print(deceptive_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10}\n",
      "Best Cross-Validation Score: 0.7390625\n",
      "Test Set Score: 0.65\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1.0, 10]\n",
    "}\n",
    "\n",
    "logistic_regression = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6500\n",
      "Precision: 0.6881\n",
      "Recall: 0.6500\n",
      "F1 Score: 0.6313\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  feature  importance\n",
      "6264        chicago hotel    0.052528\n",
      "17292       hotel chicago    0.052329\n",
      "6315   chicago millennium    0.035386\n",
      "21711        luxury hotel    0.028640\n",
      "11070          east hotel    0.025242\n",
      "\n",
      "                 feature  importance\n",
      "14159     forward using         0.0\n",
      "14160  forward vacation         0.0\n",
      "14161      forward view         0.0\n",
      "14162      forward wifi         0.0\n",
      "42435        zoo second         0.0\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = decision_tree.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)\n",
    "\n",
    "# Get the top 5 important features for deceptive reviews\n",
    "top_5_genuine_review_terms = importance_df.tail(5)  # or use .head(5) based on your label encoding\n",
    "print(\"\\n\", top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'criterion': 'entropy', 'max_depth': None, 'min_samples_leaf': 2, 'min_samples_split': 2}\n",
      "Best Cross-Validation Score: 0.7171875\n",
      "Test Set Score: 0.6375\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],  # Function to measure the quality of a split\n",
    "    'max_depth': [None, 2, 4, 6, 8, 10],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2, 4, 6]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "\n",
    "decision_tree = DecisionTreeClassifier(criterion='gini', max_depth=None, random_state=42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(decision_tree, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5938\n",
      "Precision: 0.7055\n",
      "Recall: 0.5938\n",
      "F1 Score: 0.5298\n"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "random_forest.fit(X_train, y_train)\n",
    "# predictions\n",
    "y_pred = random_forest.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average = 'weighted')\n",
    "recall = recall_score(y_test, y_pred, average = 'weighted')\n",
    "f1 = f1_score(y_test, y_pred,average = 'weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                feature  importance\n",
      "17292     hotel chicago    0.011261\n",
      "6264      chicago hotel    0.010452\n",
      "29003   recently stayed    0.008170\n",
      "32746  sheraton chicago    0.004932\n",
      "7896     conrad chicago    0.004806\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances\n",
    "importances = random_forest.feature_importances_\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualization\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Get the top 5 important features for genuine reviews\n",
    "top_5_genuine_review_terms = importance_df.head(5)  # or use .head(5) based on your label encoding\n",
    "print(top_5_genuine_review_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'sqrt', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "Best Cross-Validation Score: 0.7625\n",
      "Test Set Score: 0.68125\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],  # Number of trees in the forest\n",
    "    'max_features': [ 'sqrt', 'log2'],  # Number of features to consider for the best split\n",
    "    'max_depth': [None, 10, 20],  # Maximum depth of the tree\n",
    "    'min_samples_split': [2, 5],  # Minimum number of samples required to split an internal node\n",
    "    'min_samples_leaf': [1, 2]  # Minimum number of samples required to be at a leaf node\n",
    "}\n",
    "\n",
    "random_forest = RandomForestClassifier(criterion = 'gini', max_depth = None, random_state = 42)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(random_forest, param_grid, cv=10, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best parameters and score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_score = grid_search.score(X_test, y_test)\n",
    "print(\"Test Set Score:\", test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statystical Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
